**Part 3 ‚Äì Nucleus (Top-p) Sampling for Text Generation**


The original greedy generation method was changed to support nucleus sampling, also known as top-p sampling.

Greedy: It selects the token with the highest probability at every step. The text is very stable, but it is easy to repeat itself and become very boring.

top-p: Instead of just looking at the first place, find a "set of the top few" with a total probability ‚â• p, and then sample from it to make the sentences more varied without making it look like a completely random sampling.

In my program:
Implementing the `nucleus_sampling(logits, p)` function
I received the `top_p` parameter from the `generate_text(...)` function provided by the teacher
I actually used `p = 0.95` and `p = 1.0` to compare the generated results.

In this part, I implemented nucleus (top-p) sampling, a common decoding strategy for LLMs.
Instead of always picking the argmax token (greedy), we restrict sampling to the smallest set of tokens whose cumulative probability mass is at least p, and then sample within this set.

Algorithm concept()
Suppose that in a certain step, the model outputs the softmax probabilities of 4 tokens:
0.40, 0.30, 0.20, 0.10

We first sort them from largest to smallest: 0.40, 0.30, 0.20, 0.10
If p = 0.7:
Take 0.40 ‚Üí accumulating 0.40 < 0.7
Add 0.30 ‚Üí accumulating 0.70 ‚â• 0.7 ‚Üí stop
Therefore, this step only samples from the first two tokens (the probabilities are renormalized to 0.40/0.70 and 0.30/0.70).
If p = 0.95:
0.40 ‚Üí 0.40
+0.30 ‚Üí 0.70
+0.20 ‚Üí 0.90
+0.10 ‚Üí 1.00 ‚â• 0.95
Therefore, all four are retained, which is close to sampling over the entire softmax.

The core of top-p sampling is: First, sort the tokens ‚Üí accumulate the results ‚Üí find the smallest k such that the sum of the probabilities of the first k tokens is greater than or equal to p. Then, normalize the probabilities only among these k tokens, and finally select a token using multinomial sampling.

**Code and detailed comments**

    def nucleus_sampling(logits, p=0.95):
    """
    Implement nucleus (top-p) sampling, given the logits at a certain time step,
    and return the ID of the next token.

    parameters:
        logits: torch.Tensor, with shape (vocab_size,)
            The model's unnormalized scores for all vocabulary at a certain time step.

        p: float, between 0 and 1
            The lower bound of cumulative probability we want to retain.
            For example, when p = 0.95, we find the smallest set S such that
            the total probability mass of tokens in S is >= 0.95.

    returns:
        chosen_token_id: int
            The selected token id.
    """

    # First, restrict p to the valid range [0, 1] to avoid the caller giving strange values.
    p = float(p)
    p = max(0.0, min(1.0, p))

    # If p <= 0, it's equivalent to abandoning randomness and directly degenerating into greedy (argmax).
    # This also allows the function to have a reasonable fallback in extreme cases.
    if p <= 0.0:
        return torch.argmax(logits).item()

    # Step 1: Convert logits into a softmax probability distribution
    # Here, dim=-1 is because the shape of logits is (vocab_size,).
    probs = F.softmax(logits, dim=-1)  # shape: (vocab_size,)

    # Step 2: Sort by probability from highest to lowest.
    # sorted_probs: The sorted probability values ‚Äã‚Äã(from largest to smallest).
    # sorted_indices: The token IDs that correspond to the original vocab.
    sorted_probs, sorted_indices = torch.sort(probs, descending=True)

    # Step 3: Calculate the cumulative sum, for example, [0.4, 0.3, 0.2, 0.1] -> [0.4, 0.7, 0.9, 1.0]
    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)

    # Step 4: Find the smallest index such that cumulative_probs[index] >= p
    # This index is the k-1 we need (because it's 0-based).
    # This means that the total probability of the first (index+1) tokens has already covered p.
    cutoff_idx = torch.searchsorted(
        cumulative_probs,
        torch.tensor(p, device=logits.device)
    ).item()

    # To prevent floating-point numbers or edge cases from causing the index to go out of range, a min protection is implemented.
    cutoff_idx = min(cutoff_idx, sorted_probs.size(0) - 1)

    # Step 5: Extract the first k = cutoff_idx + 1 tokens, and these tokens form the nucleus set.
    top_probs = sorted_probs[:cutoff_idx + 1]      # The shape is approximately (k,).
    top_indices = sorted_indices[:cutoff_idx + 1]  # The shape is also (k,).

    # Step 6: Renormalize on this small set to ensure the sum of probabilities is 1.
    top_probs = top_probs / top_probs.sum()

    # Step 7: Select an index from this set using multinomial sampling.
    # sampled_index_in_top is an integer between [0..k-1].
    sampled_index_in_top = torch.multinomial(top_probs, num_samples=1).item()

    # Convert this "index in the smaller set" back to the original vocab token ID.
    chosen_token_id = top_indices[sampled_index_in_top].item()

    return chosen_token_id

This function takes in the logits from a certain step, with a shape of (vocab_size).

First, I restrict p to between 0 and 1. If p=0, it degenerates into a greedy approach.Then I perform a softmax operation on the logits to obtain probabilities. 

Next:
I use `torch.sort` to sort the probabilities from largest to smallest, obtaining the sorted probabilities and their corresponding indices.

I use `torch.cumsum` to calculate the cumulative sum and find the first position where the cumulative sum is greater than or equal to p, `cutoff_idx`.
The tokens before `cutoff_idx` form the nucleus, which is the candidate set for the top-p.

I renormalize this small set to a probability that sums to 1, and finally use `torch.multinomial` to select a token.
Therefore, this achieves the goal of "randomly selecting only from the first few reasonable tokens".


**Experimental results**

GreedyÔºö
[lstm_seq] Generating sample text (greedy) at epoch=3, step=1249...
 Greedy Sample: Once upon a time, there was a little girl named Lily. She loved to play outside in the rain. One
 Annotated: Once upon a time, there was a little girl named Lily. She loved to play outside in the rain. One

Top-p = 0.95Ôºö
[lstm_seq] Generating sample text (top-p=0.95) at epoch=3, step=1249...
 Top-p (p=0.95) Sample: Once upon a time, there was a little boy named Timmy. Timmy loved candy because his uncle had a
 Annotated: Once upon a time, there was a little boy named Timmy. Timmy loved candy because his uncle had a

Top-p = 1.0Ôºö
[lstm_seq] Generating sample text (top-p=1.0) at epoch=3, step=1249...
 Top-p (p=1.0) Sample: Once upon a time, there was a little girl called Laura. Molly was threely and she loved playing outdoors.
 Annotated: Once upon a time, there was a little girl called Laura. Molly was threely and she loved playing outdoors.

This example illustrates the following:
Greedy consistently uses the "Lily + Teddy Bear" formula, which is stable but the subject matter is very similar.
At p = 0.95, the protagonist changes (Max, Puppy), but the meaning remains reasonable.
At p = 1.0, because the entire tail is also sampled, sentences sometimes contain strange words (such as "distant sailed"), showing high diversity but less consistent quality.
This phenomenon aligns with the original theoretical expectations.


**Why is nucleus sampling needed? Can't we just use greedy sampling?**
Greedy sampling selects the token with the highest probability at each step. Its advantages are stability and predictability, but its disadvantages include high repetition rates and monotonous sentence structures.

Nucleus sampling, on the other hand, randomly selects from the "first few reasonable tokens," making the overall sentence more varied. Furthermore, because it only retains the set with a probability quality ‚â• p, it is less likely to select extremely rare or unusual tokens, thus achieving a balance between "quality" and "diversity."

**What is the difference between top-k and top-p?**
Top-k keeps a fixed set of the k most likely tokens, regardless of whether the total probability of these k tokens is 0.5 or 0.99.
Top-p (nucleus) keeps a set of tokens of varying size, provided that the total probability of this set is greater than or equal to p.
The advantage is that the set is smaller if the distribution is highly concentrated, and larger if the distribution is relatively flat.
It is more adaptive.

**What happens when p is larger or smaller?**
When p is very small (close to 0), the set may only contain 1-2 tokens, exhibiting near-greedy behavior; sentences are stable but somewhat boring.
When p is larger, the set is larger, exploring more low-probability tokens, increasing sentence diversity, but potentially leading to unusual combinations.
When p = 1.0, it's equivalent to sampling across the entire softmax, without any truncation; this is the most random and also the most dangerous case.

**How do we observe the difference between p=0.95 and p=1.0 in an experiment?**
In this TinyStories experiment,greedy almost exclusively generated sentences like "Lily is playing outside."
At p = 0.95, the protagonist and setting changed, such as Max, Timmy, a dog, or a rabbit,and most stories remained plausible.
At p = 1.0, strange words occasionally appeared, such as phrases that didn't make sense,which is in line with our expectations: because the tail wasn't truncated at all,low-probability, possibly incorrect or strange tokens were also drawn.

**Why do we need to perform softmax ourselves in nucleus_sampling instead of directly using logits?**
Because the definition of top-p is based on a probability distribution rather than raw logits.
What we need to sort are the softmax-ordered ùëù(ùëñ),and the sum of these probabilities must just exceed p.
Without performing a softmax first, it's impossible to explain the statement "total probability ‚â• p".


**Pure code:**

    def nucleus_sampling(logits, p=0.95):
    p = float(max(0.0, min(1.0, p)))
    if p <= 0.0:
        return torch.argmax(logits).item()

    probs = F.softmax(logits, dim=-1)  # (vocab_size,)

    sorted_probs, sorted_indices = torch.sort(probs, descending=True)

    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
    cutoff_idx = torch.searchsorted(
        cumulative_probs,
        torch.tensor(p, device=logits.device)
    ).item()
    cutoff_idx = min(cutoff_idx, sorted_probs.size(0) - 1)

    top_probs = sorted_probs[:cutoff_idx + 1]
    top_indices = sorted_indices[:cutoff_idx + 1]
    top_probs = top_probs / top_probs.sum()

    sampled_index_in_top = torch.multinomial(top_probs, num_samples=1).item()
    chosen_token_id = top_indices[sampled_index_in_top].item()

    return chosen_token_id

